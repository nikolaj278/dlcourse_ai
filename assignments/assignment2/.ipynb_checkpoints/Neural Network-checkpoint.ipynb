{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"../data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(train_X.shape[1], 10), train_X[:10])\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_1\n",
      "Gradient check passed!\n",
      "Checking gradient for W_2\n",
      "Gradient check passed!\n",
      "Checking gradient for B_1\n",
      "Gradient check passed!\n",
      "Checking gradient for B_2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_1\n",
      "Gradients are different at (0, 0). Analytic: 0.00518, Numeric: 0.00516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.13333333333333333)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 44.880401, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.017328, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, dataset, SGD(), learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO Implement missing pieces in Trainer.fit function\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# You should expect loss to go down every epoch, even if it's slow\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loss_history, train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dlcourse_ai/assignments/assignment2/trainer.py:105\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtrain_X[batch_indices]\n\u001b[1;32m    104\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtrain_y[batch_indices]\n\u001b[0;32m--> 105\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_and_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    108\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers[param_name]\n",
      "File \u001b[0;32m~/dlcourse_ai/assignments/assignment2/model.py:54\u001b[0m, in \u001b[0;36mTwoLayerNet.compute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m d_second \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_layer\u001b[38;5;241m.\u001b[39mbackward(d_preds)\n\u001b[1;32m     53\u001b[0m d_relu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU\u001b[38;5;241m.\u001b[39mbackward(d_second)\n\u001b[0;32m---> 54\u001b[0m d_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_relu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# params['W_2'] =  \u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# params['B_2'] = \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# params['W_1'] =  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# After that, implement l2 regularization on all params\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Hint: self.params() is useful again!\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m [params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_2\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_1\u001b[39m\u001b[38;5;124m'\u001b[39m]]: \u001b[38;5;66;03m# in params.values(): \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x126869270>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(train_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "# dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# trainer = Trainer(model, dataset, SGD(), num_epochs=10, learning_rate = 1e-2)\n",
    "# loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss: 44.679901, Train accuracy: 0.196556, val accuracy: 0.207000\n",
      "Avg. loss: 44.616111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Avg. loss: 44.546208, Train accuracy: 0.208778, val accuracy: 0.216000\n",
      "Avg. loss: 44.471103, Train accuracy: 0.220444, val accuracy: 0.215000\n",
      "Avg. loss: 44.478817, Train accuracy: 0.210333, val accuracy: 0.211000\n",
      "Avg. loss: 44.407950, Train accuracy: 0.211889, val accuracy: 0.212000\n",
      "Avg. loss: 44.427761, Train accuracy: 0.211444, val accuracy: 0.206000\n",
      "Avg. loss: 44.424328, Train accuracy: 0.221556, val accuracy: 0.219000\n",
      "Avg. loss: 44.455395, Train accuracy: 0.229111, val accuracy: 0.222000\n",
      "Avg. loss: 44.407282, Train accuracy: 0.229778, val accuracy: 0.230000\n",
      "Avg. loss: 44.442264, Train accuracy: 0.220889, val accuracy: 0.217000\n",
      "Avg. loss: 44.406702, Train accuracy: 0.220556, val accuracy: 0.202000\n",
      "Avg. loss: 44.354525, Train accuracy: 0.237333, val accuracy: 0.225000\n",
      "Avg. loss: 44.417858, Train accuracy: 0.218333, val accuracy: 0.214000\n",
      "Avg. loss: 44.396919, Train accuracy: 0.230222, val accuracy: 0.221000\n",
      "Avg. loss: 44.400688, Train accuracy: 0.227778, val accuracy: 0.221000\n",
      "Avg. loss: 44.463984, Train accuracy: 0.237111, val accuracy: 0.235000\n",
      "Avg. loss: 44.340577, Train accuracy: 0.215667, val accuracy: 0.194000\n",
      "Avg. loss: 44.348787, Train accuracy: 0.227111, val accuracy: 0.218000\n",
      "Avg. loss: 44.404893, Train accuracy: 0.235778, val accuracy: 0.228000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "# model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 10, reg = 1e-1) #hidden_layer_size = 100\n",
    "# dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# trainer = Trainer(model, dataset, MomentumSGD(), num_epochs=10\n",
    "#                   , learning_rate=1e-2, learning_rate_decay=0.99) # num_epochs=20\n",
    "\n",
    "# # You should see even better results than before!\n",
    "# loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.379631, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.271995, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: 2.803549, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 185.007620, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.333333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.333231, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.407989, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.464624, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.484530, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.028973, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 0.252611, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.906632, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.658774, Train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Loss: 2.418176, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 3.601931, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 0.175199, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 0.262965, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.270634, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 2.257466, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.714156, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.05)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.1, num_epochs=15, batch_size=2)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.286862, Train accuracy: 0.196689, val accuracy: 0.206206\n",
      "Loss: 2.264355, Train accuracy: 0.196689, val accuracy: 0.206206\n",
      "Loss: 2.106134, Train accuracy: 0.197022, val accuracy: 0.205205\n",
      "Loss: 2.370758, Train accuracy: 0.209023, val accuracy: 0.216216\n",
      "Loss: 1.998006, Train accuracy: 0.216024, val accuracy: 0.233233\n",
      "Loss: 2.004520, Train accuracy: 0.235026, val accuracy: 0.230230\n",
      "Loss: 2.190443, Train accuracy: 0.237137, val accuracy: 0.229229\n",
      "Loss: 2.171053, Train accuracy: 0.245249, val accuracy: 0.239239\n",
      "Loss: 1.962236, Train accuracy: 0.253473, val accuracy: 0.243243\n",
      "Loss: 2.113490, Train accuracy: 0.254028, val accuracy: 0.227227\n",
      "Loss: 2.293150, Train accuracy: 0.258140, val accuracy: 0.235235\n",
      "Loss: 1.845395, Train accuracy: 0.249472, val accuracy: 0.236236\n",
      "Loss: 2.053022, Train accuracy: 0.254917, val accuracy: 0.218218\n",
      "Loss: 1.804383, Train accuracy: 0.260807, val accuracy: 0.235235\n",
      "Loss: 2.211861, Train accuracy: 0.271252, val accuracy: 0.247247\n",
      "Loss: 2.294888, Train accuracy: 0.274253, val accuracy: 0.247247\n",
      "Loss: 2.078141, Train accuracy: 0.279364, val accuracy: 0.241241\n",
      "Loss: 1.947928, Train accuracy: 0.279698, val accuracy: 0.243243\n",
      "Loss: 2.118853, Train accuracy: 0.273919, val accuracy: 0.246246\n",
      "Loss: 2.180652, Train accuracy: 0.286365, val accuracy: 0.259259\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "data_size = -1\n",
    "\n",
    "\n",
    "learning_rates = 2e-3 \n",
    "reg_strength = 0.001\n",
    "learning_rate_decay = 0.7\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = TwoLayerNet(n_input=train_X.shape[1], \n",
    "                    n_output=10, \n",
    "                    hidden_layer_size=hidden_layer_size, \n",
    "                    reg=reg_strength)\n",
    "\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                    dataset=dataset, \n",
    "                    optim=SGD(),\n",
    "                    num_epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=learning_rates,\n",
    "                    learning_rate_decay=learning_rate_decay)\n",
    "\n",
    "# best_classifier = None\n",
    "# best_val_accuracy = None\n",
    "\n",
    "# loss_history = []\n",
    "# train_history = []\n",
    "# val_history = []\n",
    "\n",
    "# # TODO find the best hyperparameters to train the network\n",
    "# # Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# # You should expect to get to at least 40% of valudation accuracy\n",
    "# # Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "# print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node nr. 0, Avg loss = 2.509256793852818, train acc = 0.2646960773419269, val acc = 0.23623623623623624\n",
      "node nr. 1, Avg loss = 2.59983287073867, train acc = 0.24658295366151795, val acc = 0.22022022022022023\n",
      "node nr. 2, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 3, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 4, Avg loss = 2.50693271629473, train acc = 0.25713968218690963, val acc = 0.20520520520520522\n",
      "node nr. 5, Avg loss = 2.6065976891984524, train acc = 0.26825202800311143, val acc = 0.22022022022022023\n",
      "node nr. 6, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 7, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 8, Avg loss = 2.5113843324644605, train acc = 0.24891654628292031, val acc = 0.2122122122122122\n",
      "node nr. 9, Avg loss = 2.608330847796708, train acc = 0.2788087565285032, val acc = 0.24124124124124124\n",
      "node nr. 10, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 11, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 12, Avg loss = 2.5136219726335085, train acc = 0.2679186576286254, val acc = 0.22922922922922923\n",
      "node nr. 13, Avg loss = 2.6170540879014617, train acc = 0.27058562062451386, val acc = 0.2132132132132132\n",
      "node nr. 14, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 15, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 16, Avg loss = 2.4716214889530455, train acc = 0.2808089787754195, val acc = 0.23423423423423423\n",
      "node nr. 17, Avg loss = 2.5476932716381433, train acc = 0.2662518057561951, val acc = 0.23823823823823823\n",
      "node nr. 18, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/dlcourse_ai/assignments/assignment2/layers.py:20: RuntimeWarning: overflow encountered in square\n",
      "  loss = 0.5 * reg_strength * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node nr. 19, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 20, Avg loss = 2.4698367798693144, train acc = 0.28458717635292813, val acc = 0.22122122122122123\n",
      "node nr. 21, Avg loss = 2.548919084938237, train acc = 0.26936326258473164, val acc = 0.22522522522522523\n",
      "node nr. 22, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 23, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 24, Avg loss = 2.472037386689421, train acc = 0.26247360817868653, val acc = 0.2122122122122122\n",
      "node nr. 25, Avg loss = 2.562635309754709, train acc = 0.30270030003333703, val acc = 0.2502502502502503\n",
      "node nr. 26, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 27, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 28, Avg loss = 2.474832470663813, train acc = 0.2889209912212468, val acc = 0.24224224224224225\n",
      "node nr. 29, Avg loss = 2.559650693243216, train acc = 0.27269696632959217, val acc = 0.2182182182182182\n",
      "node nr. 30, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 31, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 32, Avg loss = 2.420466987692162, train acc = 0.2695855095010557, val acc = 0.22722722722722724\n",
      "node nr. 33, Avg loss = 2.4844241123436963, train acc = 0.28036448494277144, val acc = 0.23423423423423423\n",
      "node nr. 34, Avg loss = inf, train acc = 0.09912212468052006, val acc = 0.09509509509509509\n",
      "node nr. 35, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 36, Avg loss = 2.42745309417866, train acc = 0.2878097566396266, val acc = 0.25225225225225223\n",
      "node nr. 37, Avg loss = 2.484119256673176, train acc = 0.28125347260806755, val acc = 0.22722722722722724\n",
      "node nr. 38, Avg loss = inf, train acc = 0.09967774197133014, val acc = 0.1011011011011011\n",
      "node nr. 39, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 40, Avg loss = 2.4273570645706717, train acc = 0.27003000333370375, val acc = 0.2172172172172172\n",
      "node nr. 41, Avg loss = 2.485444130547113, train acc = 0.28936548505389487, val acc = 0.24424424424424424\n",
      "node nr. 42, Avg loss = inf, train acc = 0.0833425936215135, val acc = 0.07807807807807808\n",
      "node nr. 43, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 44, Avg loss = 2.4286557637283797, train acc = 0.2702522502500278, val acc = 0.24324324324324326\n",
      "node nr. 45, Avg loss = 2.497076842184397, train acc = 0.28725413934881655, val acc = 0.24524524524524524\n",
      "node nr. 46, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 47, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 48, Avg loss = 2.3549492874805207, train acc = 0.2835870652294699, val acc = 0.22322322322322322\n",
      "node nr. 49, Avg loss = 2.4040323724187513, train acc = 0.2912545838426492, val acc = 0.2502502502502503\n",
      "node nr. 50, Avg loss = inf, train acc = 0.10767863095899545, val acc = 0.13313313313313313\n",
      "node nr. 51, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 52, Avg loss = 2.363279153737302, train acc = 0.29303255917324145, val acc = 0.24524524524524524\n",
      "node nr. 53, Avg loss = 2.4114666764975037, train acc = 0.2956995221691299, val acc = 0.24224224224224225\n",
      "node nr. 54, Avg loss = 2.4057215628480275, train acc = 0.2958106456272919, val acc = 0.23623623623623624\n",
      "node nr. 55, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 56, Avg loss = 2.3683389759235047, train acc = 0.2920324480497833, val acc = 0.24524524524524524\n",
      "node nr. 57, Avg loss = 2.4142629123764516, train acc = 0.3018113123680409, val acc = 0.24224224224224225\n",
      "node nr. 58, Avg loss = inf, train acc = 0.1222358039782198, val acc = 0.1011011011011011\n",
      "node nr. 59, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 60, Avg loss = 2.3708335967180063, train acc = 0.29769974441604624, val acc = 0.24524524524524524\n",
      "node nr. 61, Avg loss = 2.41602982309577, train acc = 0.2956995221691299, val acc = 0.24424424424424424\n",
      "node nr. 62, Avg loss = inf, train acc = 0.1222358039782198, val acc = 0.12212212212212212\n",
      "node nr. 63, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 64, Avg loss = 2.3752453773202475, train acc = 0.24302700300033336, val acc = 0.21521521521521522\n",
      "node nr. 65, Avg loss = 2.4032943836085243, train acc = 0.2516946327369708, val acc = 0.23423423423423423\n",
      "node nr. 66, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 67, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 68, Avg loss = 2.379720690937581, train acc = 0.277919768863207, val acc = 0.22922922922922923\n",
      "node nr. 69, Avg loss = 2.4156553169383908, train acc = 0.2695855095010557, val acc = 0.22122122122122123\n",
      "node nr. 70, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 71, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 72, Avg loss = 2.3799152529583747, train acc = 0.2629181020113346, val acc = 0.23023023023023023\n",
      "node nr. 73, Avg loss = 2.4102496665344084, train acc = 0.2773641515723969, val acc = 0.24324324324324326\n",
      "node nr. 74, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 75, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 76, Avg loss = 2.378552367074486, train acc = 0.2679186576286254, val acc = 0.23123123123123124\n",
      "node nr. 77, Avg loss = 2.4176072882677264, train acc = 0.26180686742971443, val acc = 0.22122122122122123\n",
      "node nr. 78, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 79, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 80, Avg loss = 2.3376430283999414, train acc = 0.26814090454494943, val acc = 0.21921921921921922\n",
      "node nr. 81, Avg loss = 2.364480800692693, train acc = 0.28747638626514055, val acc = 0.24624624624624625\n",
      "node nr. 82, Avg loss = 2.3815315212313517, train acc = 0.2605845093899322, val acc = 0.22022022022022023\n",
      "node nr. 83, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 84, Avg loss = 2.344614406344782, train acc = 0.26591843538170906, val acc = 0.2172172172172172\n",
      "node nr. 85, Avg loss = 2.367280039147538, train acc = 0.28247583064784976, val acc = 0.24724724724724725\n",
      "node nr. 86, Avg loss = 2.374320345431927, train acc = 0.27814201577953107, val acc = 0.24124124124124124\n",
      "node nr. 87, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 88, Avg loss = 2.3431936601513286, train acc = 0.29658850983442603, val acc = 0.24624624624624625\n",
      "node nr. 89, Avg loss = 2.369157370568653, train acc = 0.2858095343927103, val acc = 0.24424424424424424\n",
      "node nr. 90, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 91, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 92, Avg loss = 2.345045220159881, train acc = 0.2809201022335815, val acc = 0.23723723723723725\n",
      "node nr. 93, Avg loss = 2.3691800140602095, train acc = 0.2793643738193133, val acc = 0.24524524524524524\n",
      "node nr. 94, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 95, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 96, Avg loss = 2.2971581085692185, train acc = 0.29847760862318035, val acc = 0.24424424424424424\n",
      "node nr. 97, Avg loss = 2.31365837519294, train acc = 0.27925325036115123, val acc = 0.22722722722722724\n",
      "node nr. 98, Avg loss = 2.3195712700181237, train acc = 0.2809201022335815, val acc = 0.24024024024024024\n",
      "node nr. 99, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 100, Avg loss = 2.302362952492014, train acc = 0.30025558395377266, val acc = 0.25925925925925924\n",
      "node nr. 101, Avg loss = 2.3174362552562924, train acc = 0.2828092010223358, val acc = 0.24024024024024024\n",
      "node nr. 102, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 103, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 104, Avg loss = 2.298581899445261, train acc = 0.2758084231581287, val acc = 0.22322322322322322\n",
      "node nr. 105, Avg loss = 2.3175137915937856, train acc = 0.29103233692632513, val acc = 0.24824824824824826\n",
      "node nr. 106, Avg loss = inf, train acc = 0.12768085342815869, val acc = 0.13013013013013014\n",
      "node nr. 107, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 108, Avg loss = 2.3030168191272593, train acc = 0.29103233692632513, val acc = 0.24024024024024024\n",
      "node nr. 109, Avg loss = 2.3273238580458906, train acc = 0.2982553617068563, val acc = 0.24724724724724725\n",
      "node nr. 110, Avg loss = inf, train acc = 0.09401044560506723, val acc = 0.09009009009009009\n",
      "node nr. 111, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 112, Avg loss = 2.2480985543326764, train acc = 0.28858762084676076, val acc = 0.22922922922922923\n",
      "node nr. 113, Avg loss = 2.258533552037997, train acc = 0.27925325036115123, val acc = 0.23023023023023023\n",
      "node nr. 114, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 115, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 116, Avg loss = 2.250461470044647, train acc = 0.30670074452716967, val acc = 0.25725725725725723\n",
      "node nr. 117, Avg loss = 2.2612558953948176, train acc = 0.29214357150794534, val acc = 0.24724724724724725\n",
      "node nr. 118, Avg loss = inf, train acc = 0.17179686631847982, val acc = 0.17917917917917917\n",
      "node nr. 119, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 120, Avg loss = 2.253881846350138, train acc = 0.2858095343927103, val acc = 0.23623623623623624\n",
      "node nr. 121, Avg loss = 2.2687391284385376, train acc = 0.2871430158906545, val acc = 0.22522522522522523\n",
      "node nr. 122, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 123, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 124, Avg loss = 2.2540747360467126, train acc = 0.308589843315924, val acc = 0.25825825825825827\n",
      "node nr. 125, Avg loss = 2.2654104283784524, train acc = 0.3030336704078231, val acc = 0.23923923923923923\n",
      "node nr. 126, Avg loss = 2.266574773828081, train acc = 0.28725413934881655, val acc = 0.24824824824824826\n",
      "node nr. 127, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 128, Avg loss = 2.2481232891051453, train acc = 0.26336259584398264, val acc = 0.22122122122122123\n",
      "node nr. 129, Avg loss = 2.259351326953833, train acc = 0.26802978108678743, val acc = 0.22322322322322322\n",
      "node nr. 130, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 131, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 132, Avg loss = 2.246393272447851, train acc = 0.2759195466162907, val acc = 0.24724724724724725\n",
      "node nr. 133, Avg loss = 2.260204009261226, train acc = 0.26191799088787643, val acc = 0.21521521521521522\n",
      "node nr. 134, Avg loss = 2.2732581314289657, train acc = 0.2581397933103678, val acc = 0.21521521521521522\n",
      "node nr. 135, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 136, Avg loss = 2.250168751980276, train acc = 0.27003000333370375, val acc = 0.24624624624624625\n",
      "node nr. 137, Avg loss = 2.2581737497982046, train acc = 0.24591621291254584, val acc = 0.2132132132132132\n",
      "node nr. 138, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 139, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 140, Avg loss = 2.250247870059244, train acc = 0.26014001555728417, val acc = 0.22022022022022023\n",
      "node nr. 141, Avg loss = 2.2625090282715714, train acc = 0.26591843538170906, val acc = 0.2222222222222222\n",
      "node nr. 142, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 143, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 144, Avg loss = 2.2182573985416387, train acc = 0.2871430158906545, val acc = 0.23923923923923923\n",
      "node nr. 145, Avg loss = 2.2236223388099767, train acc = 0.2565840648960996, val acc = 0.22822822822822822\n",
      "node nr. 146, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 147, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 148, Avg loss = 2.2182068764493246, train acc = 0.2760306700744527, val acc = 0.24124124124124124\n",
      "node nr. 149, Avg loss = 2.227184794903826, train acc = 0.2788087565285032, val acc = 0.23923923923923923\n",
      "node nr. 150, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 151, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 152, Avg loss = 2.221454825913861, train acc = 0.27014112679186575, val acc = 0.23023023023023023\n",
      "node nr. 153, Avg loss = 2.228767942973577, train acc = 0.28325369485498386, val acc = 0.23723723723723725\n",
      "node nr. 154, Avg loss = 2.2325531370846003, train acc = 0.2683631514612735, val acc = 0.24424424424424424\n",
      "node nr. 155, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 156, Avg loss = 2.223801211889996, train acc = 0.28803200355595066, val acc = 0.23823823823823823\n",
      "node nr. 157, Avg loss = 2.2287162096032005, train acc = 0.28880986776308476, val acc = 0.25325325325325326\n",
      "node nr. 158, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 159, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 160, Avg loss = 2.1898314204636407, train acc = 0.2894766085120569, val acc = 0.24424424424424424\n",
      "node nr. 161, Avg loss = 2.193574453439371, train acc = 0.29414379375486166, val acc = 0.25125125125125125\n",
      "node nr. 162, Avg loss = 2.197077227704742, train acc = 0.27836426269585507, val acc = 0.25725725725725723\n",
      "node nr. 163, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 164, Avg loss = 2.190798933117368, train acc = 0.2828092010223358, val acc = 0.22122122122122123\n",
      "node nr. 165, Avg loss = 2.199518743338561, train acc = 0.29725525058339813, val acc = 0.24724724724724725\n",
      "node nr. 166, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 167, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 168, Avg loss = 2.1883187189422406, train acc = 0.28792088009778866, val acc = 0.25225225225225223\n",
      "node nr. 169, Avg loss = 2.1974475212109144, train acc = 0.3005889543282587, val acc = 0.24824824824824826\n",
      "node nr. 170, Avg loss = inf, train acc = 0.07734192688076454, val acc = 0.07307307307307308\n",
      "node nr. 171, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 172, Avg loss = 2.1910294554030942, train acc = 0.27925325036115123, val acc = 0.2222222222222222\n",
      "node nr. 173, Avg loss = 2.1958087621606737, train acc = 0.2919213245916213, val acc = 0.23123123123123124\n",
      "node nr. 174, Avg loss = inf, train acc = 0.09789976664073786, val acc = 0.08408408408408409\n",
      "node nr. 175, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 176, Avg loss = 2.159148857775637, train acc = 0.271807978664296, val acc = 0.23223223223223224\n",
      "node nr. 177, Avg loss = 2.1618902002869547, train acc = 0.2906989665518391, val acc = 0.24024024024024024\n",
      "node nr. 178, Avg loss = inf, train acc = 0.14957217468607623, val acc = 0.16916916916916916\n",
      "node nr. 179, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 180, Avg loss = 2.1567298985191483, train acc = 0.28369818868763197, val acc = 0.22622622622622623\n",
      "node nr. 181, Avg loss = 2.1623097703830423, train acc = 0.29536615179464387, val acc = 0.26126126126126126\n",
      "node nr. 182, Avg loss = inf, train acc = 0.15757306367374152, val acc = 0.17117117117117117\n",
      "node nr. 183, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 184, Avg loss = 2.1582271150264254, train acc = 0.2855872874763863, val acc = 0.23123123123123124\n",
      "node nr. 185, Avg loss = 2.1633986029599663, train acc = 0.3029225469496611, val acc = 0.25725725725725723\n",
      "node nr. 186, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 187, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 188, Avg loss = 2.1601912362813906, train acc = 0.29414379375486166, val acc = 0.25425425425425424\n",
      "node nr. 189, Avg loss = 2.1635266988283215, train acc = 0.3043671519057673, val acc = 0.24524524524524524\n",
      "node nr. 190, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 191, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 192, Avg loss = 2.1914546489026034, train acc = 0.28936548505389487, val acc = 0.25625625625625625\n",
      "node nr. 193, Avg loss = 2.2030518987811574, train acc = 0.26269585509501053, val acc = 0.21921921921921922\n",
      "node nr. 194, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 195, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 196, Avg loss = 2.1933535636413035, train acc = 0.2815868429825536, val acc = 0.25425425425425424\n",
      "node nr. 197, Avg loss = 2.1998419234468902, train acc = 0.2733637070785643, val acc = 0.23723723723723725\n",
      "node nr. 198, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 199, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 200, Avg loss = 2.1938795315720627, train acc = 0.2510278919879987, val acc = 0.2032032032032032\n",
      "node nr. 201, Avg loss = 2.202261441197724, train acc = 0.265473941549061, val acc = 0.21621621621621623\n",
      "node nr. 202, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 203, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 204, Avg loss = 2.1927680648195, train acc = 0.28592065785087234, val acc = 0.24024024024024024\n",
      "node nr. 205, Avg loss = 2.2011355683470653, train acc = 0.25069452161351263, val acc = 0.22122122122122123\n",
      "node nr. 206, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 207, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 208, Avg loss = 2.168498249804794, train acc = 0.2751416824091566, val acc = 0.24324324324324326\n",
      "node nr. 209, Avg loss = 2.170936941992503, train acc = 0.2788087565285032, val acc = 0.2172172172172172\n",
      "node nr. 210, Avg loss = 2.1830495190029673, train acc = 0.2599177686409601, val acc = 0.23523523523523523\n",
      "node nr. 211, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 212, Avg loss = 2.1709652969736317, train acc = 0.28803200355595066, val acc = 0.23023023023023023\n",
      "node nr. 213, Avg loss = 2.176363200972828, train acc = 0.28192021335703965, val acc = 0.23223223223223224\n",
      "node nr. 214, Avg loss = 2.18595735613503, train acc = 0.29236581842426934, val acc = 0.24124124124124124\n",
      "node nr. 215, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 216, Avg loss = 2.1685634555205597, train acc = 0.26025113901544616, val acc = 0.22022022022022023\n",
      "node nr. 217, Avg loss = 2.174435326473797, train acc = 0.27980886765196133, val acc = 0.23923923923923923\n",
      "node nr. 218, Avg loss = 2.18777310502521, train acc = 0.2574730525613957, val acc = 0.2172172172172172\n",
      "node nr. 219, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 220, Avg loss = 2.16985417982425, train acc = 0.2938104233803756, val acc = 0.22122122122122123\n",
      "node nr. 221, Avg loss = 2.175114976220988, train acc = 0.28125347260806755, val acc = 0.24124124124124124\n",
      "node nr. 222, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 223, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 224, Avg loss = 2.147707348878977, train acc = 0.28880986776308476, val acc = 0.24724724724724725\n",
      "node nr. 225, Avg loss = 2.15131111297981, train acc = 0.28603178130903434, val acc = 0.22522522522522523\n",
      "node nr. 226, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 227, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 228, Avg loss = 2.147374913713677, train acc = 0.28792088009778866, val acc = 0.23323323323323322\n",
      "node nr. 229, Avg loss = 2.149332949889108, train acc = 0.29425491721302366, val acc = 0.26226226226226224\n",
      "node nr. 230, Avg loss = 2.1523677592714403, train acc = 0.2966996332925881, val acc = 0.24924924924924924\n",
      "node nr. 231, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 232, Avg loss = 2.1470122075296234, train acc = 0.2938104233803756, val acc = 0.2552552552552553\n",
      "node nr. 233, Avg loss = 2.1504427063118157, train acc = 0.28814312701411265, val acc = 0.24124124124124124\n",
      "node nr. 234, Avg loss = 2.15555071007131, train acc = 0.28792088009778866, val acc = 0.22922922922922923\n",
      "node nr. 235, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 236, Avg loss = 2.1454389281626103, train acc = 0.2974774974997222, val acc = 0.23723723723723725\n",
      "node nr. 237, Avg loss = 2.149569859594557, train acc = 0.2912545838426492, val acc = 0.24124124124124124\n",
      "node nr. 238, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 239, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 240, Avg loss = 2.1237302614834976, train acc = 0.3029225469496611, val acc = 0.24824824824824826\n",
      "node nr. 241, Avg loss = 2.1243399144592985, train acc = 0.2943660406711857, val acc = 0.25325325325325326\n",
      "node nr. 242, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 243, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 244, Avg loss = 2.120652071046301, train acc = 0.290143349261029, val acc = 0.23423423423423423\n",
      "node nr. 245, Avg loss = 2.1227985560129365, train acc = 0.2820313368152017, val acc = 0.23023023023023023\n",
      "node nr. 246, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 247, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 248, Avg loss = 2.1230204634052443, train acc = 0.31003444827203025, val acc = 0.24824824824824826\n",
      "node nr. 249, Avg loss = 2.1247089876216765, train acc = 0.28525391710190023, val acc = 0.24024024024024024\n",
      "node nr. 250, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 251, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 252, Avg loss = 2.1240471157616705, train acc = 0.3005889543282587, val acc = 0.24524524524524524\n",
      "node nr. 253, Avg loss = 2.122367589283983, train acc = 0.2884764973885987, val acc = 0.22422422422422422\n",
      "node nr. 254, Avg loss = 2.1274297865817906, train acc = 0.30114457161906877, val acc = 0.24424424424424424\n",
      "node nr. 255, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 256, Avg loss = 2.1629667659636636, train acc = 0.2731414601622402, val acc = 0.22422422422422422\n",
      "node nr. 257, Avg loss = 2.174223730554286, train acc = 0.2599177686409601, val acc = 0.22922922922922923\n",
      "node nr. 258, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 259, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 260, Avg loss = 2.160267492353557, train acc = 0.2605845093899322, val acc = 0.23123123123123124\n",
      "node nr. 261, Avg loss = 2.1656334849937413, train acc = 0.2605845093899322, val acc = 0.22622622622622623\n",
      "node nr. 262, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 263, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 264, Avg loss = 2.161782798430817, train acc = 0.27214134903878207, val acc = 0.23423423423423423\n",
      "node nr. 265, Avg loss = 2.1690819696763106, train acc = 0.2574730525613957, val acc = 0.23923923923923923\n",
      "node nr. 266, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 267, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 268, Avg loss = 2.161032691820364, train acc = 0.2634737193021447, val acc = 0.2092092092092092\n",
      "node nr. 269, Avg loss = 2.1722010694016323, train acc = 0.2919213245916213, val acc = 0.23323323323323322\n",
      "node nr. 270, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 271, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 272, Avg loss = 2.1452186041366175, train acc = 0.27547505278364265, val acc = 0.2032032032032032\n",
      "node nr. 273, Avg loss = 2.147744978027462, train acc = 0.29292143571507945, val acc = 0.2602602602602603\n",
      "node nr. 274, Avg loss = 2.1645887178885195, train acc = 0.26391821313479275, val acc = 0.24424424424424424\n",
      "node nr. 275, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 276, Avg loss = 2.1429938920442337, train acc = 0.2732525836204023, val acc = 0.22822822822822822\n",
      "node nr. 277, Avg loss = 2.14815138482873, train acc = 0.25769529947771974, val acc = 0.21921921921921922\n",
      "node nr. 278, Avg loss = 2.161775785744756, train acc = 0.24636070674519392, val acc = 0.2182182182182182\n",
      "node nr. 279, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 280, Avg loss = 2.1424011125152296, train acc = 0.2835870652294699, val acc = 0.24124124124124124\n",
      "node nr. 281, Avg loss = 2.145421659113927, train acc = 0.2765862873652628, val acc = 0.23323323323323322\n",
      "node nr. 282, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 283, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 284, Avg loss = 2.1445053347604643, train acc = 0.27214134903878207, val acc = 0.24924924924924924\n",
      "node nr. 285, Avg loss = 2.1458929556391033, train acc = 0.26880764529392154, val acc = 0.23523523523523523\n",
      "node nr. 286, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 287, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 288, Avg loss = 2.1243238681292578, train acc = 0.302478053117013, val acc = 0.23923923923923923\n",
      "node nr. 289, Avg loss = 2.1318415263119848, train acc = 0.28658739859984445, val acc = 0.23823823823823823\n",
      "node nr. 290, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 291, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 292, Avg loss = 2.1243630414724293, train acc = 0.2926991887987554, val acc = 0.23523523523523523\n",
      "node nr. 293, Avg loss = 2.1275643068647625, train acc = 0.2846982998110901, val acc = 0.23323323323323322\n",
      "node nr. 294, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 295, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 296, Avg loss = 2.1225459762723022, train acc = 0.28058673185909544, val acc = 0.22122122122122123\n",
      "node nr. 297, Avg loss = 2.1287183684853224, train acc = 0.29858873208134235, val acc = 0.25325325325325326\n",
      "node nr. 298, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 299, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 300, Avg loss = 2.1230642791061736, train acc = 0.2851427936437382, val acc = 0.22622622622622623\n",
      "node nr. 301, Avg loss = 2.1230102045196624, train acc = 0.31003444827203025, val acc = 0.24024024024024024\n",
      "node nr. 302, Avg loss = 2.1317192468547774, train acc = 0.2813645960662296, val acc = 0.2222222222222222\n",
      "node nr. 303, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 304, Avg loss = 2.104139054255635, train acc = 0.28792088009778866, val acc = 0.24424424424424424\n",
      "node nr. 305, Avg loss = 2.105380106850222, train acc = 0.3057006334037115, val acc = 0.26426426426426425\n",
      "node nr. 306, Avg loss = inf, train acc = 0.11201244582731415, val acc = 0.11911911911911911\n",
      "node nr. 307, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 308, Avg loss = 2.102180016470129, train acc = 0.31203467051894657, val acc = 0.25325325325325326\n",
      "node nr. 309, Avg loss = 2.104017811469635, train acc = 0.3001444604956106, val acc = 0.24724724724724725\n",
      "node nr. 310, Avg loss = inf, train acc = 0.07556395155017225, val acc = 0.06306306306306306\n",
      "node nr. 311, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 312, Avg loss = 2.1042111435897444, train acc = 0.2850316701855762, val acc = 0.23723723723723725\n",
      "node nr. 313, Avg loss = 2.104842124034949, train acc = 0.3018113123680409, val acc = 0.25325325325325326\n",
      "node nr. 314, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 315, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 316, Avg loss = 2.102130549333377, train acc = 0.30036670741193466, val acc = 0.23923923923923923\n",
      "node nr. 317, Avg loss = 2.104138276842616, train acc = 0.3012556950772308, val acc = 0.24524524524524524\n",
      "node nr. 318, Avg loss = 2.1105813608053947, train acc = 0.29414379375486166, val acc = 0.25225225225225223\n",
      "node nr. 319, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 320, Avg loss = 2.358485695539302, train acc = 0.2815868429825536, val acc = 0.24824824824824826\n",
      "node nr. 321, Avg loss = 2.3959664629872783, train acc = 0.2869207689743305, val acc = 0.23923923923923923\n",
      "node nr. 322, Avg loss = 2.415810680084099, train acc = 0.28369818868763197, val acc = 0.23523523523523523\n",
      "node nr. 323, Avg loss = 2.4158400289840873, train acc = 0.26658517613068117, val acc = 0.2222222222222222\n",
      "node nr. 324, Avg loss = 2.364449656106835, train acc = 0.2856984109345483, val acc = 0.25225225225225223\n",
      "node nr. 325, Avg loss = 2.3981598458904645, train acc = 0.289809978886543, val acc = 0.24124124124124124\n",
      "node nr. 326, Avg loss = 2.4189784667061427, train acc = 0.26814090454494943, val acc = 0.24024024024024024\n",
      "node nr. 327, Avg loss = 2.4306955821793674, train acc = 0.2758084231581287, val acc = 0.22822822822822822\n",
      "node nr. 328, Avg loss = 2.3704026701963494, train acc = 0.28792088009778866, val acc = 0.26226226226226224\n",
      "node nr. 329, Avg loss = 2.4048540345116445, train acc = 0.28303144793865986, val acc = 0.23623623623623624\n",
      "node nr. 330, Avg loss = 2.4247330273649585, train acc = 0.28125347260806755, val acc = 0.23323323323323322\n",
      "node nr. 331, Avg loss = 2.4305731384471723, train acc = 0.28047560840093344, val acc = 0.24324324324324326\n",
      "node nr. 332, Avg loss = 2.3761802258996987, train acc = 0.2848094232692521, val acc = 0.25325325325325326\n",
      "node nr. 333, Avg loss = 2.4110502834012757, train acc = 0.27991999111012333, val acc = 0.24124124124124124\n",
      "node nr. 334, Avg loss = 2.434089543084918, train acc = 0.2856984109345483, val acc = 0.25625625625625625\n",
      "node nr. 335, Avg loss = 2.443849234472537, train acc = 0.28458717635292813, val acc = 0.2502502502502503\n",
      "node nr. 336, Avg loss = 2.328367588313507, train acc = 0.27436381820202244, val acc = 0.21921921921921922\n",
      "node nr. 337, Avg loss = 2.353830726232876, train acc = 0.2938104233803756, val acc = 0.25225225225225223\n",
      "node nr. 338, Avg loss = 2.374757438856634, train acc = 0.2871430158906545, val acc = 0.22922922922922923\n",
      "node nr. 339, Avg loss = 2.3744267749013686, train acc = 0.27058562062451386, val acc = 0.23623623623623624\n",
      "node nr. 340, Avg loss = 2.3334229942240934, train acc = 0.27425269474386044, val acc = 0.24224224224224225\n",
      "node nr. 341, Avg loss = 2.35746021506322, train acc = 0.28814312701411265, val acc = 0.24024024024024024\n",
      "node nr. 342, Avg loss = 2.374372274720455, train acc = 0.29003222580286697, val acc = 0.24924924924924924\n",
      "node nr. 343, Avg loss = 2.381804810280207, train acc = 0.2788087565285032, val acc = 0.23423423423423423\n",
      "node nr. 344, Avg loss = 2.3376070914025817, train acc = 0.28458717635292813, val acc = 0.24424424424424424\n",
      "node nr. 345, Avg loss = 2.365377526368757, train acc = 0.29958884320480056, val acc = 0.25825825825825827\n",
      "node nr. 346, Avg loss = 2.3783722032792163, train acc = 0.2702522502500278, val acc = 0.22622622622622623\n",
      "node nr. 347, Avg loss = 2.392353569951614, train acc = 0.2753639293254806, val acc = 0.23123123123123124\n",
      "node nr. 348, Avg loss = 2.3407674519499895, train acc = 0.2734748305367263, val acc = 0.23723723723723725\n",
      "node nr. 349, Avg loss = 2.371520081707424, train acc = 0.29169907767529724, val acc = 0.23123123123123124\n",
      "node nr. 350, Avg loss = 2.385871259685226, train acc = 0.2740304478275364, val acc = 0.23123123123123124\n",
      "node nr. 351, Avg loss = 2.3936329540074124, train acc = 0.2896988554283809, val acc = 0.23723723723723725\n",
      "node nr. 352, Avg loss = 2.2944548010879853, train acc = 0.2745860651183465, val acc = 0.22422422422422422\n",
      "node nr. 353, Avg loss = 2.3145915041433742, train acc = 0.28536504056006223, val acc = 0.24924924924924924\n",
      "node nr. 354, Avg loss = 2.324466971039458, train acc = 0.2969218802089121, val acc = 0.23923923923923923\n",
      "node nr. 355, Avg loss = 2.326686147798181, train acc = 0.29492165796199576, val acc = 0.25325325325325326\n",
      "node nr. 356, Avg loss = 2.2980849583361964, train acc = 0.2858095343927103, val acc = 0.23923923923923923\n",
      "node nr. 357, Avg loss = 2.320991632135789, train acc = 0.29425491721302366, val acc = 0.2552552552552553\n",
      "node nr. 358, Avg loss = 2.3323994941316446, train acc = 0.29103233692632513, val acc = 0.25225225225225223\n",
      "node nr. 359, Avg loss = 2.3328560393850655, train acc = 0.29414379375486166, val acc = 0.25625625625625625\n",
      "node nr. 360, Avg loss = 2.304060496709516, train acc = 0.2993665962884765, val acc = 0.25725725725725723\n",
      "node nr. 361, Avg loss = 2.3238987243947222, train acc = 0.2746971885765085, val acc = 0.2222222222222222\n",
      "node nr. 362, Avg loss = 2.3348845949794663, train acc = 0.2904767196355151, val acc = 0.25425425425425424\n",
      "node nr. 363, Avg loss = 2.334176287705286, train acc = 0.29703300366707414, val acc = 0.25825825825825827\n",
      "node nr. 364, Avg loss = 2.3072078995260257, train acc = 0.29725525058339813, val acc = 0.25925925925925924\n",
      "node nr. 365, Avg loss = 2.330878808655285, train acc = 0.29092121346816313, val acc = 0.22822822822822822\n",
      "node nr. 366, Avg loss = 2.3393692331300167, train acc = 0.290365596177353, val acc = 0.24724724724724725\n",
      "node nr. 367, Avg loss = 2.3459440378003222, train acc = 0.2911434603844872, val acc = 0.25425425425425424\n",
      "node nr. 368, Avg loss = 2.258247814656496, train acc = 0.2973663740415602, val acc = 0.25625625625625625\n",
      "node nr. 369, Avg loss = 2.2711697462926805, train acc = 0.2814757195243916, val acc = 0.23423423423423423\n",
      "node nr. 370, Avg loss = 2.2800069345692155, train acc = 0.2988109789976664, val acc = 0.2552552552552553\n",
      "node nr. 371, Avg loss = 2.2801955455993195, train acc = 0.2919213245916213, val acc = 0.25325325325325326\n",
      "node nr. 372, Avg loss = 2.26252532701139, train acc = 0.29725525058339813, val acc = 0.2502502502502503\n",
      "node nr. 373, Avg loss = 2.275421929255718, train acc = 0.29236581842426934, val acc = 0.24724724724724725\n",
      "node nr. 374, Avg loss = 2.2817657930323985, train acc = 0.2912545838426492, val acc = 0.24424424424424424\n",
      "node nr. 375, Avg loss = 2.2870301462546596, train acc = 0.2980331147905323, val acc = 0.24824824824824826\n",
      "node nr. 376, Avg loss = 2.2641143419910494, train acc = 0.29647738637626403, val acc = 0.25325325325325326\n",
      "node nr. 377, Avg loss = 2.281509765352799, train acc = 0.2851427936437382, val acc = 0.23523523523523523\n",
      "node nr. 378, Avg loss = 2.2884213465035192, train acc = 0.2982553617068563, val acc = 0.25625625625625625\n",
      "node nr. 379, Avg loss = 2.2880052822432555, train acc = 0.29414379375486166, val acc = 0.24224224224224225\n",
      "node nr. 380, Avg loss = 2.2676722363880284, train acc = 0.29647738637626403, val acc = 0.24924924924924924\n",
      "node nr. 381, Avg loss = 2.285152499086243, train acc = 0.29625513945994, val acc = 0.24824824824824826\n",
      "node nr. 382, Avg loss = 2.2942878920121883, train acc = 0.2890321146794088, val acc = 0.24224224224224225\n",
      "node nr. 383, Avg loss = 2.2999284330306025, train acc = 0.29792199133237024, val acc = 0.24824824824824826\n",
      "node nr. 384, Avg loss = 2.2552818002018697, train acc = 0.2871430158906545, val acc = 0.24424424424424424\n",
      "node nr. 385, Avg loss = 2.2633013406438964, train acc = 0.2793643738193133, val acc = 0.23423423423423423\n",
      "node nr. 386, Avg loss = 2.2722072565252596, train acc = 0.28314257139682186, val acc = 0.25225225225225223\n",
      "node nr. 387, Avg loss = 2.274901099018859, train acc = 0.28325369485498386, val acc = 0.2552552552552553\n",
      "node nr. 388, Avg loss = 2.253462246278266, train acc = 0.2800311145682854, val acc = 0.23823823823823823\n",
      "node nr. 389, Avg loss = 2.267243479883456, train acc = 0.2834759417713079, val acc = 0.24124124124124124\n",
      "node nr. 390, Avg loss = 2.2730910050972595, train acc = 0.2869207689743305, val acc = 0.24424424424424424\n",
      "node nr. 391, Avg loss = 2.278485061904587, train acc = 0.2814757195243916, val acc = 0.22922922922922923\n",
      "node nr. 392, Avg loss = 2.2582235081893725, train acc = 0.29425491721302366, val acc = 0.2502502502502503\n",
      "node nr. 393, Avg loss = 2.2672763945987384, train acc = 0.28058673185909544, val acc = 0.24124124124124124\n",
      "node nr. 394, Avg loss = 2.2828188852974547, train acc = 0.2876986331814646, val acc = 0.24124124124124124\n",
      "node nr. 395, Avg loss = 2.2886367343351077, train acc = 0.28125347260806755, val acc = 0.23323323323323322\n",
      "node nr. 396, Avg loss = 2.2618669388640424, train acc = 0.2932548060895655, val acc = 0.24724724724724725\n",
      "node nr. 397, Avg loss = 2.2678503819968827, train acc = 0.2764751639071008, val acc = 0.24424424424424424\n",
      "node nr. 398, Avg loss = 2.2801417297049795, train acc = 0.27825313923769307, val acc = 0.23323323323323322\n",
      "node nr. 399, Avg loss = 2.284900140292662, train acc = 0.2982553617068563, val acc = 0.25925925925925924\n",
      "node nr. 400, Avg loss = 2.2291118476163994, train acc = 0.2856984109345483, val acc = 0.23523523523523523\n",
      "node nr. 401, Avg loss = 2.2366574463793363, train acc = 0.28236470718968776, val acc = 0.23823823823823823\n",
      "node nr. 402, Avg loss = 2.242746686634091, train acc = 0.29347705300588955, val acc = 0.24824824824824826\n",
      "node nr. 403, Avg loss = 2.246419200072072, train acc = 0.2746971885765085, val acc = 0.22722722722722724\n",
      "node nr. 404, Avg loss = 2.233443201940034, train acc = 0.2806978553172575, val acc = 0.24424424424424424\n",
      "node nr. 405, Avg loss = 2.2415505957882735, train acc = 0.29492165796199576, val acc = 0.24024024024024024\n",
      "node nr. 406, Avg loss = 2.244372273531489, train acc = 0.28880986776308476, val acc = 0.23923923923923923\n",
      "node nr. 407, Avg loss = 2.2493755576650942, train acc = 0.290143349261029, val acc = 0.25325325325325326\n",
      "node nr. 408, Avg loss = 2.231661878434694, train acc = 0.29158795421713524, val acc = 0.24024024024024024\n",
      "node nr. 409, Avg loss = 2.24162791348532, train acc = 0.278030892321369, val acc = 0.23323323323323322\n",
      "node nr. 410, Avg loss = 2.251157753749512, train acc = 0.28380931214579397, val acc = 0.24024024024024024\n",
      "node nr. 411, Avg loss = 2.2541686553367795, train acc = 0.28725413934881655, val acc = 0.24124124124124124\n",
      "node nr. 412, Avg loss = 2.237273017183493, train acc = 0.290254472719191, val acc = 0.23923923923923923\n",
      "node nr. 413, Avg loss = 2.2430677780458903, train acc = 0.2980331147905323, val acc = 0.24924924924924924\n",
      "node nr. 414, Avg loss = 2.251315506797596, train acc = 0.29536615179464387, val acc = 0.24824824824824826\n",
      "node nr. 415, Avg loss = 2.2572633012179937, train acc = 0.2863651516835204, val acc = 0.23223223223223224\n",
      "node nr. 416, Avg loss = 2.2042169195968553, train acc = 0.28925436159573287, val acc = 0.24324324324324326\n",
      "node nr. 417, Avg loss = 2.2121930950618056, train acc = 0.3005889543282587, val acc = 0.26326326326326327\n",
      "node nr. 418, Avg loss = 2.2126935232325637, train acc = 0.27747527503055897, val acc = 0.2172172172172172\n",
      "node nr. 419, Avg loss = 2.2191297441728537, train acc = 0.27991999111012333, val acc = 0.23223223223223224\n",
      "node nr. 420, Avg loss = 2.206994092633225, train acc = 0.3012556950772308, val acc = 0.25925925925925924\n",
      "node nr. 421, Avg loss = 2.2125864274961677, train acc = 0.284031559062118, val acc = 0.23823823823823823\n",
      "node nr. 422, Avg loss = 2.2170027390871914, train acc = 0.2800311145682854, val acc = 0.24524524524524524\n",
      "node nr. 423, Avg loss = 2.2207248926203205, train acc = 0.28658739859984445, val acc = 0.23523523523523523\n",
      "node nr. 424, Avg loss = 2.208125141322646, train acc = 0.2969218802089121, val acc = 0.24724724724724725\n",
      "node nr. 425, Avg loss = 2.2156980180255657, train acc = 0.28925436159573287, val acc = 0.25725725725725723\n",
      "node nr. 426, Avg loss = 2.2199984785134474, train acc = 0.3005889543282587, val acc = 0.25425425425425424\n",
      "node nr. 427, Avg loss = 2.2192012988833123, train acc = 0.296032892543616, val acc = 0.24924924924924924\n",
      "node nr. 428, Avg loss = 2.211622780647698, train acc = 0.30447827536392935, val acc = 0.25825825825825827\n",
      "node nr. 429, Avg loss = 2.21839239396082, train acc = 0.29703300366707414, val acc = 0.24624624624624625\n",
      "node nr. 430, Avg loss = 2.2229654914312635, train acc = 0.3005889543282587, val acc = 0.25825825825825827\n",
      "node nr. 431, Avg loss = 2.2270046970933612, train acc = 0.2925880653405934, val acc = 0.23323323323323322\n",
      "node nr. 432, Avg loss = 2.1805461964836983, train acc = 0.29425491721302366, val acc = 0.24224224224224225\n",
      "node nr. 433, Avg loss = 2.183982125174312, train acc = 0.29558839871096787, val acc = 0.25725725725725723\n",
      "node nr. 434, Avg loss = 2.1853684630360974, train acc = 0.29658850983442603, val acc = 0.25125125125125125\n",
      "node nr. 435, Avg loss = 2.18683078707034, train acc = 0.2856984109345483, val acc = 0.23523523523523523\n",
      "node nr. 436, Avg loss = 2.1813299220711757, train acc = 0.29547727525280587, val acc = 0.23623623623623624\n",
      "node nr. 437, Avg loss = 2.185542973623308, train acc = 0.2988109789976664, val acc = 0.2502502502502503\n",
      "node nr. 438, Avg loss = 2.189456577243377, train acc = 0.2931436826314035, val acc = 0.24224224224224225\n",
      "node nr. 439, Avg loss = 2.191286543399391, train acc = 0.28936548505389487, val acc = 0.24524524524524524\n",
      "node nr. 440, Avg loss = 2.1844514127942216, train acc = 0.29781086787420824, val acc = 0.24924924924924924\n",
      "node nr. 441, Avg loss = 2.189524559256212, train acc = 0.30514501611290146, val acc = 0.25625625625625625\n",
      "node nr. 442, Avg loss = 2.191621110239438, train acc = 0.2986998555395044, val acc = 0.24624624624624625\n",
      "node nr. 443, Avg loss = 2.1886320393189016, train acc = 0.28925436159573287, val acc = 0.23923923923923923\n",
      "node nr. 444, Avg loss = 2.184219768087751, train acc = 0.2982553617068563, val acc = 0.25125125125125125\n",
      "node nr. 445, Avg loss = 2.1906146713185137, train acc = 0.29769974441604624, val acc = 0.24924924924924924\n",
      "node nr. 446, Avg loss = 2.1946273089985446, train acc = 0.29703300366707414, val acc = 0.24824824824824826\n",
      "node nr. 447, Avg loss = 2.196209264036073, train acc = 0.296144016001778, val acc = 0.2602602602602603\n",
      "node nr. 448, Avg loss = 2.17099828381435, train acc = 0.28658739859984445, val acc = 0.23823823823823823\n",
      "node nr. 449, Avg loss = 2.175675753884072, train acc = 0.2966996332925881, val acc = 0.26626626626626626\n",
      "node nr. 450, Avg loss = 2.1797389884316747, train acc = 0.28814312701411265, val acc = 0.24724724724724725\n",
      "node nr. 451, Avg loss = 2.186614640209456, train acc = 0.2725858428714302, val acc = 0.24524524524524524\n",
      "node nr. 452, Avg loss = 2.172568657607338, train acc = 0.29636626291810203, val acc = 0.26126126126126126\n",
      "node nr. 453, Avg loss = 2.1776926473277887, train acc = 0.28858762084676076, val acc = 0.24724724724724725\n",
      "node nr. 454, Avg loss = 2.1790281585116054, train acc = 0.27991999111012333, val acc = 0.22522522522522523\n",
      "node nr. 455, Avg loss = 2.1818445050550874, train acc = 0.2918102011334593, val acc = 0.26226226226226224\n",
      "node nr. 456, Avg loss = 2.170990168478582, train acc = 0.29158795421713524, val acc = 0.23023023023023023\n",
      "node nr. 457, Avg loss = 2.175480170139538, train acc = 0.28869874430492276, val acc = 0.24324324324324326\n",
      "node nr. 458, Avg loss = 2.180742509617108, train acc = 0.27980886765196133, val acc = 0.24324324324324326\n",
      "node nr. 459, Avg loss = 2.1908744777488622, train acc = 0.2834759417713079, val acc = 0.24324324324324326\n",
      "node nr. 460, Avg loss = 2.174746317548706, train acc = 0.2876986331814646, val acc = 0.24924924924924924\n",
      "node nr. 461, Avg loss = 2.1782145835733253, train acc = 0.28725413934881655, val acc = 0.2502502502502503\n",
      "node nr. 462, Avg loss = 2.183668820970206, train acc = 0.2856984109345483, val acc = 0.24324324324324326\n",
      "node nr. 463, Avg loss = 2.1869110932723883, train acc = 0.28258695410601176, val acc = 0.24124124124124124\n",
      "node nr. 464, Avg loss = 2.1582593796998477, train acc = 0.3031447938659851, val acc = 0.26426426426426425\n",
      "node nr. 465, Avg loss = 2.160154088283135, train acc = 0.2856984109345483, val acc = 0.23123123123123124\n",
      "node nr. 466, Avg loss = 2.1632274336710915, train acc = 0.29403267029669966, val acc = 0.2502502502502503\n",
      "node nr. 467, Avg loss = 2.1675634415156746, train acc = 0.2802533614846094, val acc = 0.23123123123123124\n",
      "node nr. 468, Avg loss = 2.1567792279107634, train acc = 0.29169907767529724, val acc = 0.24624624624624625\n",
      "node nr. 469, Avg loss = 2.1614312007568963, train acc = 0.290365596177353, val acc = 0.24024024024024024\n",
      "node nr. 470, Avg loss = 2.16557919965169, train acc = 0.29903322591399045, val acc = 0.2602602602602603\n",
      "node nr. 471, Avg loss = 2.168139106527875, train acc = 0.28614290476719634, val acc = 0.22722722722722724\n",
      "node nr. 472, Avg loss = 2.1575147180132976, train acc = 0.2969218802089121, val acc = 0.25125125125125125\n",
      "node nr. 473, Avg loss = 2.1586345339313517, train acc = 0.28592065785087234, val acc = 0.24924924924924924\n",
      "node nr. 474, Avg loss = 2.1642276060541477, train acc = 0.28180908989887765, val acc = 0.23423423423423423\n",
      "node nr. 475, Avg loss = 2.166589268368176, train acc = 0.2870318924324925, val acc = 0.24524524524524524\n",
      "node nr. 476, Avg loss = 2.1586101448053596, train acc = 0.2666962995888432, val acc = 0.21421421421421422\n",
      "node nr. 477, Avg loss = 2.162273317091741, train acc = 0.29169907767529724, val acc = 0.24424424424424424\n",
      "node nr. 478, Avg loss = 2.164633104306295, train acc = 0.28814312701411265, val acc = 0.23423423423423423\n",
      "node nr. 479, Avg loss = 2.168450015152516, train acc = 0.289921102344705, val acc = 0.26126126126126126\n",
      "node nr. 480, Avg loss = 2.142079622002644, train acc = 0.2882542504722747, val acc = 0.23923923923923923\n",
      "node nr. 481, Avg loss = 2.142690531012067, train acc = 0.296032892543616, val acc = 0.24424424424424424\n",
      "node nr. 482, Avg loss = 2.1484290173571186, train acc = 0.2894766085120569, val acc = 0.25725725725725723\n",
      "node nr. 483, Avg loss = 2.1516661142579263, train acc = 0.2938104233803756, val acc = 0.24424424424424424\n",
      "node nr. 484, Avg loss = 2.143901592697073, train acc = 0.29281031225691745, val acc = 0.24124124124124124\n",
      "node nr. 485, Avg loss = 2.145009382044606, train acc = 0.2931436826314035, val acc = 0.24424424424424424\n",
      "node nr. 486, Avg loss = 2.1481003089507933, train acc = 0.30258917657517503, val acc = 0.26226226226226224\n",
      "node nr. 487, Avg loss = 2.145848925974902, train acc = 0.284031559062118, val acc = 0.24324324324324326\n",
      "node nr. 488, Avg loss = 2.1408205660841433, train acc = 0.30092232470274477, val acc = 0.2552552552552553\n",
      "node nr. 489, Avg loss = 2.145680031872767, train acc = 0.29103233692632513, val acc = 0.24424424424424424\n",
      "node nr. 490, Avg loss = 2.148746666305242, train acc = 0.28169796644071565, val acc = 0.23623623623623624\n",
      "node nr. 491, Avg loss = 2.150596786870979, train acc = 0.2973663740415602, val acc = 0.25925925925925924\n",
      "node nr. 492, Avg loss = 2.144007315699455, train acc = 0.3031447938659851, val acc = 0.24224224224224225\n",
      "node nr. 493, Avg loss = 2.146257037301414, train acc = 0.29769974441604624, val acc = 0.23723723723723725\n",
      "node nr. 494, Avg loss = 2.147682033348611, train acc = 0.2856984109345483, val acc = 0.24024024024024024\n",
      "node nr. 495, Avg loss = 2.1525594149703906, train acc = 0.29214357150794534, val acc = 0.24624624624624625\n",
      "node nr. 496, Avg loss = 2.1289521744658386, train acc = 0.29969996666296256, val acc = 0.24524524524524524\n",
      "node nr. 497, Avg loss = 2.1303921909313965, train acc = 0.29558839871096787, val acc = 0.24524524524524524\n",
      "node nr. 498, Avg loss = 2.132263598662108, train acc = 0.29647738637626403, val acc = 0.24124124124124124\n",
      "node nr. 499, Avg loss = 2.1354602257441284, train acc = 0.2986998555395044, val acc = 0.2602602602602603\n",
      "node nr. 500, Avg loss = 2.1276917855705384, train acc = 0.29981109012112456, val acc = 0.24724724724724725\n",
      "node nr. 501, Avg loss = 2.1295471827821046, train acc = 0.3013668185353928, val acc = 0.25825825825825827\n",
      "node nr. 502, Avg loss = 2.130482882244102, train acc = 0.2924769418824314, val acc = 0.24724724724724725\n",
      "node nr. 503, Avg loss = 2.1335867306283673, train acc = 0.2986998555395044, val acc = 0.25425425425425424\n",
      "node nr. 504, Avg loss = 2.127211243679412, train acc = 0.2982553617068563, val acc = 0.2502502502502503\n",
      "node nr. 505, Avg loss = 2.130179465746709, train acc = 0.2993665962884765, val acc = 0.25325325325325326\n",
      "node nr. 506, Avg loss = 2.1323217747063143, train acc = 0.30447827536392935, val acc = 0.26626626626626626\n",
      "node nr. 507, Avg loss = 2.13585720919742, train acc = 0.2956995221691299, val acc = 0.25125125125125125\n",
      "node nr. 508, Avg loss = 2.1256035152145776, train acc = 0.29225469496610734, val acc = 0.24324324324324326\n",
      "node nr. 509, Avg loss = 2.128085598220928, train acc = 0.3017001889098789, val acc = 0.25925925925925924\n",
      "node nr. 510, Avg loss = 2.13119455937137, train acc = 0.2980331147905323, val acc = 0.26126126126126126\n",
      "node nr. 511, Avg loss = 2.1338392338015426, train acc = 0.289921102344705, val acc = 0.24824824824824826\n",
      "node nr. 512, Avg loss = 2.1413587372876512, train acc = 0.3015890654517169, val acc = 0.24224224224224225\n",
      "node nr. 513, Avg loss = 2.144427962287664, train acc = 0.27691965773974886, val acc = 0.24324324324324326\n",
      "node nr. 514, Avg loss = 2.149106481505023, train acc = 0.2863651516835204, val acc = 0.24624624624624625\n",
      "node nr. 515, Avg loss = 2.150875496843889, train acc = 0.2843649294366041, val acc = 0.23523523523523523\n",
      "node nr. 516, Avg loss = 2.1407310760637035, train acc = 0.2871430158906545, val acc = 0.22522522522522523\n",
      "node nr. 517, Avg loss = 2.1426401866712674, train acc = 0.29081009001000113, val acc = 0.24624624624624625\n",
      "node nr. 518, Avg loss = 2.148379942608104, train acc = 0.278030892321369, val acc = 0.22822822822822822\n",
      "node nr. 519, Avg loss = 2.158589259775163, train acc = 0.2813645960662296, val acc = 0.25825825825825827\n",
      "node nr. 520, Avg loss = 2.1388550238670367, train acc = 0.296032892543616, val acc = 0.2682682682682683\n",
      "node nr. 521, Avg loss = 2.13986222437314, train acc = 0.27691965773974886, val acc = 0.21521521521521522\n",
      "node nr. 522, Avg loss = 2.1480842917166645, train acc = 0.2793643738193133, val acc = 0.2552552552552553\n",
      "node nr. 523, Avg loss = 2.1570883833871934, train acc = 0.2905878430936771, val acc = 0.24524524524524524\n",
      "node nr. 524, Avg loss = 2.140573804999859, train acc = 0.3014779419935548, val acc = 0.24924924924924924\n",
      "node nr. 525, Avg loss = 2.1397736340043605, train acc = 0.2894766085120569, val acc = 0.23123123123123124\n",
      "node nr. 526, Avg loss = 2.1480199018204402, train acc = 0.28614290476719634, val acc = 0.24624624624624625\n",
      "node nr. 527, Avg loss = 2.1528537732544755, train acc = 0.2926991887987554, val acc = 0.26226226226226224\n",
      "node nr. 528, Avg loss = 2.1277211319420295, train acc = 0.28880986776308476, val acc = 0.25225225225225223\n",
      "node nr. 529, Avg loss = 2.1314712510497174, train acc = 0.30103344816090677, val acc = 0.23923923923923923\n",
      "node nr. 530, Avg loss = 2.133798668002902, train acc = 0.27480831203467054, val acc = 0.22322322322322322\n",
      "node nr. 531, Avg loss = 2.1467585129459383, train acc = 0.2906989665518391, val acc = 0.24824824824824826\n",
      "node nr. 532, Avg loss = 2.128656445876418, train acc = 0.29169907767529724, val acc = 0.23623623623623624\n",
      "node nr. 533, Avg loss = 2.133556504002602, train acc = 0.2870318924324925, val acc = 0.24724724724724725\n",
      "node nr. 534, Avg loss = 2.133622224298797, train acc = 0.277919768863207, val acc = 0.22522522522522523\n",
      "node nr. 535, Avg loss = 2.1421306867967598, train acc = 0.2829203244804978, val acc = 0.23523523523523523\n",
      "node nr. 536, Avg loss = 2.1317990699252283, train acc = 0.27758639848872096, val acc = 0.23323323323323322\n",
      "node nr. 537, Avg loss = 2.129105334308307, train acc = 0.2723635959551061, val acc = 0.22622622622622623\n",
      "node nr. 538, Avg loss = 2.1403315667418066, train acc = 0.2802533614846094, val acc = 0.22322322322322322\n",
      "node nr. 539, Avg loss = 2.1385880770328267, train acc = 0.2882542504722747, val acc = 0.24824824824824826\n",
      "node nr. 540, Avg loss = 2.1287954377695693, train acc = 0.28803200355595066, val acc = 0.23423423423423423\n",
      "node nr. 541, Avg loss = 2.130287712943499, train acc = 0.2884764973885987, val acc = 0.24624624624624625\n",
      "node nr. 542, Avg loss = 2.134654307927523, train acc = 0.2814757195243916, val acc = 0.24924924924924924\n",
      "node nr. 543, Avg loss = 2.1401551683515563, train acc = 0.29903322591399045, val acc = 0.25325325325325326\n",
      "node nr. 544, Avg loss = 2.1195378775388787, train acc = 0.29358817646405155, val acc = 0.25425425425425424\n",
      "node nr. 545, Avg loss = 2.120097797258834, train acc = 0.29425491721302366, val acc = 0.24824824824824826\n",
      "node nr. 546, Avg loss = 2.1262431648741096, train acc = 0.2944771641293477, val acc = 0.24924924924924924\n",
      "node nr. 547, Avg loss = 2.1257091646061927, train acc = 0.29714412712523613, val acc = 0.25425425425425424\n",
      "node nr. 548, Avg loss = 2.1163890287422458, train acc = 0.28792088009778866, val acc = 0.24424424424424424\n",
      "node nr. 549, Avg loss = 2.122327027529727, train acc = 0.29725525058339813, val acc = 0.24224224224224225\n",
      "node nr. 550, Avg loss = 2.12257630845237, train acc = 0.29369929992221355, val acc = 0.24124124124124124\n",
      "node nr. 551, Avg loss = 2.1240108591617033, train acc = 0.2813645960662296, val acc = 0.23423423423423423\n",
      "node nr. 552, Avg loss = 2.1185851147340387, train acc = 0.30336704078230914, val acc = 0.25125125125125125\n",
      "node nr. 553, Avg loss = 2.1197253141286327, train acc = 0.2919213245916213, val acc = 0.24624624624624625\n",
      "node nr. 554, Avg loss = 2.1213101397506424, train acc = 0.3007000777864207, val acc = 0.26226226226226224\n",
      "node nr. 555, Avg loss = 2.1268089276712927, train acc = 0.2956995221691299, val acc = 0.24524524524524524\n",
      "node nr. 556, Avg loss = 2.1173115261017026, train acc = 0.29281031225691745, val acc = 0.24624624624624625\n",
      "node nr. 557, Avg loss = 2.120392923798024, train acc = 0.28792088009778866, val acc = 0.22722722722722724\n",
      "node nr. 558, Avg loss = 2.1256063384121764, train acc = 0.2975886209578842, val acc = 0.24924924924924924\n",
      "node nr. 559, Avg loss = 2.1276683471630826, train acc = 0.29425491721302366, val acc = 0.25225225225225223\n",
      "node nr. 560, Avg loss = 2.1088812375509676, train acc = 0.29858873208134235, val acc = 0.24624624624624625\n",
      "node nr. 561, Avg loss = 2.110266652713702, train acc = 0.2925880653405934, val acc = 0.25725725725725723\n",
      "node nr. 562, Avg loss = 2.1110393094478694, train acc = 0.2969218802089121, val acc = 0.24924924924924924\n",
      "node nr. 563, Avg loss = 2.1161912745722256, train acc = 0.29147683075897324, val acc = 0.24124124124124124\n",
      "node nr. 564, Avg loss = 2.1080130466259144, train acc = 0.29903322591399045, val acc = 0.24924924924924924\n",
      "node nr. 565, Avg loss = 2.1088044557506906, train acc = 0.29303255917324145, val acc = 0.25425425425425424\n",
      "node nr. 566, Avg loss = 2.1108794530279087, train acc = 0.2986998555395044, val acc = 0.25625625625625625\n",
      "node nr. 567, Avg loss = 2.117203728042952, train acc = 0.29836648516501835, val acc = 0.24724724724724725\n",
      "node nr. 568, Avg loss = 2.106233325934211, train acc = 0.29725525058339813, val acc = 0.24924924924924924\n",
      "node nr. 569, Avg loss = 2.1099263709815244, train acc = 0.3035892876986332, val acc = 0.2652652652652653\n",
      "node nr. 570, Avg loss = 2.1105269596149223, train acc = 0.28858762084676076, val acc = 0.24324324324324326\n",
      "node nr. 571, Avg loss = 2.1119389422243713, train acc = 0.29547727525280587, val acc = 0.2552552552552553\n",
      "node nr. 572, Avg loss = 2.106656930633046, train acc = 0.30036670741193466, val acc = 0.24224224224224225\n",
      "node nr. 573, Avg loss = 2.1067989549884913, train acc = 0.3072563618179798, val acc = 0.25825825825825827\n",
      "node nr. 574, Avg loss = 2.1099595502804247, train acc = 0.29558839871096787, val acc = 0.25125125125125125\n",
      "node nr. 575, Avg loss = 2.114920851220309, train acc = 0.28325369485498386, val acc = 0.23123123123123124\n",
      "node nr. 576, Avg loss = 2.12465847661917, train acc = 0.2933659295477275, val acc = 0.25225225225225223\n",
      "node nr. 577, Avg loss = 2.1258516425355345, train acc = 0.277919768863207, val acc = 0.24824824824824826\n",
      "node nr. 578, Avg loss = 2.1305416897894287, train acc = 0.28536504056006223, val acc = 0.2602602602602603\n",
      "node nr. 579, Avg loss = 2.1388529233447224, train acc = 0.27625291699077675, val acc = 0.24224224224224225\n",
      "node nr. 580, Avg loss = 2.1249424527191225, train acc = 0.29958884320480056, val acc = 0.26326326326326327\n",
      "node nr. 581, Avg loss = 2.127772932766483, train acc = 0.2813645960662296, val acc = 0.24824824824824826\n",
      "node nr. 582, Avg loss = 2.1340294993885607, train acc = 0.29403267029669966, val acc = 0.24924924924924924\n",
      "node nr. 583, Avg loss = 2.141064953879632, train acc = 0.29147683075897324, val acc = 0.25825825825825827\n",
      "node nr. 584, Avg loss = 2.1226581588188353, train acc = 0.29403267029669966, val acc = 0.23523523523523523\n",
      "node nr. 585, Avg loss = 2.1261963276521083, train acc = 0.27636404044893875, val acc = 0.21121121121121122\n",
      "node nr. 586, Avg loss = 2.1338324800795023, train acc = 0.2785865096121791, val acc = 0.23123123123123124\n",
      "node nr. 587, Avg loss = 2.1410835606421106, train acc = 0.28169796644071565, val acc = 0.23923923923923923\n",
      "node nr. 588, Avg loss = 2.123969729736814, train acc = 0.2895877319702189, val acc = 0.22722722722722724\n",
      "node nr. 589, Avg loss = 2.125804252555003, train acc = 0.28792088009778866, val acc = 0.23223223223223224\n",
      "node nr. 590, Avg loss = 2.133005559401975, train acc = 0.2878097566396266, val acc = 0.24124124124124124\n",
      "node nr. 591, Avg loss = 2.139434475465045, train acc = 0.2724747194132681, val acc = 0.22722722722722724\n",
      "node nr. 592, Avg loss = 2.114422410903109, train acc = 0.2882542504722747, val acc = 0.24824824824824826\n",
      "node nr. 593, Avg loss = 2.1189094829059143, train acc = 0.2785865096121791, val acc = 0.23723723723723725\n",
      "node nr. 594, Avg loss = 2.12191933200508, train acc = 0.2919213245916213, val acc = 0.24724724724724725\n",
      "node nr. 595, Avg loss = 2.1288033679655514, train acc = 0.2729192132459162, val acc = 0.24124124124124124\n",
      "node nr. 596, Avg loss = 2.1176785642557965, train acc = 0.29714412712523613, val acc = 0.22322322322322322\n",
      "node nr. 597, Avg loss = 2.1193005060878596, train acc = 0.29636626291810203, val acc = 0.24924924924924924\n",
      "node nr. 598, Avg loss = 2.1240121243754286, train acc = 0.2906989665518391, val acc = 0.23323323323323322\n",
      "node nr. 599, Avg loss = 2.1266273686042925, train acc = 0.2875875097233026, val acc = 0.25625625625625625\n",
      "node nr. 600, Avg loss = 2.1155825864426028, train acc = 0.2992554728303145, val acc = 0.2502502502502503\n",
      "node nr. 601, Avg loss = 2.1187041933053194, train acc = 0.28536504056006223, val acc = 0.25125125125125125\n",
      "node nr. 602, Avg loss = 2.124984257031019, train acc = 0.27836426269585507, val acc = 0.24024024024024024\n",
      "node nr. 603, Avg loss = 2.1313570782085356, train acc = 0.28669852205800644, val acc = 0.24224224224224225\n",
      "node nr. 604, Avg loss = 2.115212771278533, train acc = 0.278030892321369, val acc = 0.22422422422422422\n",
      "node nr. 605, Avg loss = 2.117347748915836, train acc = 0.29081009001000113, val acc = 0.24624624624624625\n",
      "node nr. 606, Avg loss = 2.1213226716586275, train acc = 0.29658850983442603, val acc = 0.24924924924924924\n",
      "node nr. 607, Avg loss = 2.1256515667071683, train acc = 0.29558839871096787, val acc = 0.2502502502502503\n",
      "node nr. 608, Avg loss = 2.105651532932474, train acc = 0.29725525058339813, val acc = 0.25325325325325326\n",
      "node nr. 609, Avg loss = 2.1096781992611717, train acc = 0.2952550283364818, val acc = 0.25225225225225223\n",
      "node nr. 610, Avg loss = 2.114711706668839, train acc = 0.2905878430936771, val acc = 0.24224224224224225\n",
      "node nr. 611, Avg loss = 2.1170393603141973, train acc = 0.2722524724969441, val acc = 0.22822822822822822\n",
      "node nr. 612, Avg loss = 2.1087719908820683, train acc = 0.29969996666296256, val acc = 0.24724724724724725\n",
      "node nr. 613, Avg loss = 2.1110570722212105, train acc = 0.2863651516835204, val acc = 0.24924924924924924\n",
      "node nr. 614, Avg loss = 2.1130536583547848, train acc = 0.2944771641293477, val acc = 0.24624624624624625\n",
      "node nr. 615, Avg loss = 2.116253428781579, train acc = 0.2966996332925881, val acc = 0.25825825825825827\n",
      "node nr. 616, Avg loss = 2.10437473039283, train acc = 0.30581175686187356, val acc = 0.24724724724724725\n",
      "node nr. 617, Avg loss = 2.1077118205899597, train acc = 0.29769974441604624, val acc = 0.24924924924924924\n",
      "node nr. 618, Avg loss = 2.110418501469663, train acc = 0.2956995221691299, val acc = 0.24624624624624625\n",
      "node nr. 619, Avg loss = 2.1136364534423597, train acc = 0.29636626291810203, val acc = 0.24424424424424424\n",
      "node nr. 620, Avg loss = 2.1049662217169387, train acc = 0.2850316701855762, val acc = 0.22722722722722724\n",
      "node nr. 621, Avg loss = 2.109004739108554, train acc = 0.29481053450383377, val acc = 0.24824824824824826\n",
      "node nr. 622, Avg loss = 2.1098645057983747, train acc = 0.3035892876986332, val acc = 0.25725725725725723\n",
      "node nr. 623, Avg loss = 2.121637636695154, train acc = 0.302366929658851, val acc = 0.24724724724724725\n",
      "node nr. 624, Avg loss = 2.0992315950819957, train acc = 0.2933659295477275, val acc = 0.25225225225225223\n",
      "node nr. 625, Avg loss = 2.101443045244348, train acc = 0.2975886209578842, val acc = 0.25725725725725723\n",
      "node nr. 626, Avg loss = 2.1038638515314045, train acc = 0.29536615179464387, val acc = 0.25925925925925924\n",
      "node nr. 627, Avg loss = 2.1060831704131067, train acc = 0.29725525058339813, val acc = 0.23823823823823823\n",
      "node nr. 628, Avg loss = 2.099620454979295, train acc = 0.29847760862318035, val acc = 0.25125125125125125\n",
      "node nr. 629, Avg loss = 2.099639963268713, train acc = 0.3031447938659851, val acc = 0.2552552552552553\n",
      "node nr. 630, Avg loss = 2.101352515363932, train acc = 0.3012556950772308, val acc = 0.25725725725725723\n",
      "node nr. 631, Avg loss = 2.105174744805352, train acc = 0.28592065785087234, val acc = 0.24524524524524524\n",
      "node nr. 632, Avg loss = 2.0953978889121943, train acc = 0.3029225469496611, val acc = 0.25425425425425424\n",
      "node nr. 633, Avg loss = 2.098797506683716, train acc = 0.29481053450383377, val acc = 0.22822822822822822\n",
      "node nr. 634, Avg loss = 2.100595728721477, train acc = 0.30281142349149903, val acc = 0.2602602602602603\n",
      "node nr. 635, Avg loss = 2.104882256566387, train acc = 0.2911434603844872, val acc = 0.24524524524524524\n",
      "node nr. 636, Avg loss = 2.0949502391862316, train acc = 0.30281142349149903, val acc = 0.24824824824824826\n",
      "node nr. 637, Avg loss = 2.0968043178403186, train acc = 0.3000333370374486, val acc = 0.24624624624624625\n",
      "node nr. 638, Avg loss = 2.099979516398384, train acc = 0.29836648516501835, val acc = 0.26126126126126126\n",
      "node nr. 639, Avg loss = 2.104129927434946, train acc = 0.29703300366707414, val acc = 0.2552552552552553\n",
      "node nr. 640, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 641, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 642, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 643, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 644, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 645, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 646, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 647, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 648, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 649, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 650, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 651, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 652, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 653, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 654, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 655, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 656, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 657, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 658, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 659, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 660, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 661, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 662, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 663, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 664, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 665, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 666, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 667, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 668, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 669, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 670, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 671, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 672, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 673, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 674, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 675, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 676, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 677, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 678, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 679, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 680, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 681, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 682, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 683, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 684, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 685, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 686, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 687, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 688, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 689, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 690, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 691, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 692, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 693, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 694, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 695, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 696, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 697, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 698, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 699, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 700, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 701, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 702, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 703, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 704, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 705, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 706, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 707, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 708, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 709, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 710, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 711, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 712, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 713, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 714, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 715, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 716, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 717, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 718, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 719, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 720, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 721, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 722, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 723, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 724, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 725, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 726, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 727, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 728, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 729, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 730, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 731, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 732, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 733, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 734, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 735, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 736, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 737, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 738, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 739, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 740, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 741, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 742, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 743, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 744, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 745, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 746, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 747, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 748, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 749, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 750, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 751, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 752, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 753, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 754, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 755, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 756, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 757, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 758, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 759, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 760, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 761, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 762, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 763, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 764, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 765, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 766, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 767, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 768, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 769, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 770, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 771, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 772, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 773, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 774, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 775, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 776, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 777, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 778, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 779, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 780, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 781, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 782, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 783, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 784, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 785, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 786, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 787, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 788, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 789, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 790, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 791, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 792, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 793, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 794, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 795, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 796, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 797, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 798, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 799, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 800, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 801, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 802, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 803, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 804, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 805, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 806, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 807, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 808, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 809, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 810, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 811, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 812, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 813, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 814, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 815, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 816, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 817, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 818, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 819, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 820, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 821, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 822, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 823, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 824, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 825, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 826, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 827, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 828, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 829, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 830, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 831, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 832, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 833, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 834, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 835, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 836, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 837, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 838, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 839, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 840, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 841, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 842, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 843, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 844, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 845, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 846, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 847, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 848, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 849, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 850, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 851, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 852, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 853, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 854, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 855, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 856, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 857, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 858, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 859, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 860, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 861, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 862, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 863, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 864, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 865, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 866, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 867, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 868, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 869, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 870, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 871, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 872, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 873, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 874, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 875, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 876, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 877, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 878, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 879, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 880, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 881, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 882, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 883, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 884, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 885, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 886, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 887, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 888, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 889, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 890, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 891, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 892, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 893, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 894, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 895, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 896, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 897, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 898, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 899, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 900, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 901, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 902, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 903, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 904, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 905, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 906, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 907, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 908, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 909, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 910, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 911, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 912, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 913, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 914, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 915, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 916, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 917, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 918, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 919, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 920, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 921, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 922, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 923, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 924, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 925, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 926, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 927, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 928, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 929, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 930, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 931, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 932, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 933, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 934, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 935, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 936, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 937, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 938, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 939, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 940, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 941, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 942, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 943, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 944, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 945, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 946, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 947, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n",
      "node nr. 948, Avg loss = nan, train acc = 0.06634070452272475, val acc = 0.04904904904904905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport itertools as it\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdata_size = -1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mlr = [1e-2, 5e-3, 3e-2, 2e-3, 1e-3, 5e-4, 2e-4]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mrs = [0.01, 0.005, 0.002, 0.001, 0.0005]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mlrd = [0.9, 0.7, 0.5, 0.3]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhls = [32, 64, 128, 256]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbs = [16, 32, 64, 96]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mparam_grid = list(it.product(lr, rs, lrd, hls, bs))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnum_epochs = 20\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnode_results = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfor i, node in enumerate(param_grid):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    learning_rates, reg_strength, learning_rate_decay, hidden_layer_size, batch_size = node\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model = TwoLayerNet(n_input=train_X.shape[1], \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        n_output=10, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        hidden_layer_size=hidden_layer_size, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        reg=reg_strength)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer = Trainer(model=model,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        dataset=dataset, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        optim=SGD(),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        num_epochs=num_epochs,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        batch_size=batch_size,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        learning_rate=learning_rates,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                        learning_rate_decay=learning_rate_decay)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    loss_history, train_acc_history, val_acc_history = trainer.fit()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    node_results.append((loss_history, train_acc_history, val_acc_history))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    print(f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnode nr. \u001b[39;49m\u001b[38;5;132;43;01m{i}\u001b[39;49;00m\u001b[38;5;124;43m, Avg loss = \u001b[39;49m\u001b[38;5;132;43;01m{loss_history[-1]}\u001b[39;49;00m\u001b[38;5;124;43m, train acc = \u001b[39;49m\u001b[38;5;132;43;01m{train_acc_history[-1]}\u001b[39;49;00m\u001b[38;5;124;43m, val acc = \u001b[39;49m\u001b[38;5;132;43;01m{val_acc_history[-1]}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vw_py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/vw_py310/lib/python3.10/site-packages/IPython/core/magics/execution.py:1370\u001b[0m, in \u001b[0;36mExecutionMagics.time\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1368\u001b[0m st \u001b[38;5;241m=\u001b[39m clock2()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "File \u001b[0;32m<timed exec>:35\u001b[0m\n",
      "File \u001b[0;32m~/dlcourse_ai/assignments/assignment2/trainer.py:105\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtrain_X[batch_indices]\n\u001b[1;32m    104\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtrain_y[batch_indices]\n\u001b[0;32m--> 105\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_and_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    108\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers[param_name]\n",
      "File \u001b[0;32m~/dlcourse_ai/assignments/assignment2/model.py:64\u001b[0m, in \u001b[0;36mTwoLayerNet.compute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# params['W_2'] =  \u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# params['B_2'] = \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# params['W_1'] =  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# After that, implement l2 regularization on all params\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Hint: self.params() is useful again!\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m [params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_2\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW_1\u001b[39m\u001b[38;5;124m'\u001b[39m]]: \u001b[38;5;66;03m# in params.values(): \u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     param_loss, param_grad \u001b[38;5;241m=\u001b[39m \u001b[43ml2_regularization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     65\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_grad\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_loss\n",
      "File \u001b[0;32m~/dlcourse_ai/assignments/assignment2/layers.py:20\u001b[0m, in \u001b[0;36ml2_regularization\u001b[0;34m(W, reg_strength)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mComputes L2 regularization loss on weights and its gradient\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m  gradient, np.array same shape as W - gradient of weight by l2 loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# TODO: Copy from the previous assignment\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# loss = reg_strength * np.sum(W**2)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# grad = 2 * reg_strength * W\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m reg_strength \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m grad \u001b[38;5;241m=\u001b[39m reg_strength \u001b[38;5;241m*\u001b[39m W\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[0;32m~/miniconda3/envs/vw_py310/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:2466\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vw_py310/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "data_size = -1\n",
    "\n",
    "lr = [1e-2, 5e-3, 3e-2, 2e-3, 1e-3, 5e-4, 2e-4]\n",
    "rs = [0.01, 0.005, 0.002, 0.001, 0.0005]\n",
    "lrd = [0.9, 0.7, 0.5, 0.3]\n",
    "hls = [32, 64, 128, 256]\n",
    "bs = [16, 32, 64, 96]\n",
    "\n",
    "param_grid = list(it.product(lr, rs, lrd, hls, bs))\n",
    "num_epochs = 20\n",
    "node_results = []\n",
    "\n",
    "for i, node in enumerate(param_grid):\n",
    "    learning_rates, reg_strength, learning_rate_decay, hidden_layer_size, batch_size = node\n",
    "\n",
    "    \n",
    "    model = TwoLayerNet(n_input=train_X.shape[1], \n",
    "                        n_output=10, \n",
    "                        hidden_layer_size=hidden_layer_size, \n",
    "                        reg=reg_strength)\n",
    "    \n",
    "    dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "    \n",
    "    trainer = Trainer(model=model,\n",
    "                        dataset=dataset, \n",
    "                        optim=SGD(),\n",
    "                        num_epochs=num_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=learning_rates,\n",
    "                        learning_rate_decay=learning_rate_decay)\n",
    "    \n",
    "\n",
    "    loss_history, train_acc_history, val_acc_history = trainer.fit()\n",
    "    \n",
    "    node_results.append((loss_history, train_acc_history, val_acc_history))\n",
    "    \n",
    "    print(f'node nr. {i}, Avg loss = {loss_history[-1]}, train acc = {train_acc_history[-1]}, val acc = {val_acc_history[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['loss_history', 'train_acc_history', 'val_acc_history']\n",
    "res_dict = {}\n",
    "for i, res in enumerate(node_results):\n",
    "    res_dict.update({i: {name: values for name, values in zip(names, res)}})\n",
    "    \n",
    "with open('results.pkl', 'wb') as file:\n",
    "    pkl.dump(res_dict, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'rb') as file:\n",
    "    dict = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23649051, 2.23261629, 2.24226386, 2.25675468, 2.27389826,\n",
       "       2.29463013, 2.31417732, 2.33377943, 2.35041837, 2.3686808 ,\n",
       "       2.38084282, 2.39272407, 2.41114055, 2.42379724, 2.43795915,\n",
       "       2.4517796 , 2.46810287, 2.48060926, 2.4961093 , 2.50925679])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict[0]['loss_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12820f700>]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
